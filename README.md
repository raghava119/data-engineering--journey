# data-engineering--journey

Welcome! This repository documents my journey learning Data Engineering â€” tools, frameworks, projects and daily commits.

## ðŸŽ¯ Goal
Iâ€™m committed to **daily practice and documentation** of data engineering topics, in order to build real-world skills and a portfolio that demonstrates progress over time.

## ðŸ“‚ Folder Structure
Hereâ€™s how the folders are organised:
- `01_AWS/` â€” Core AWS services relevant to data engineering (S3, Glue, Lambda, Redshift, etc)  
- `02_Databricks/` â€” Databricks notebooks, Delta Lake experiments, Unity Catalog demos  
- `03_Snowflake/` â€” Snowflake data cloud: stages, loading, querying, performance  
- `04_SQL/` â€” SQL fundamentals, interview queries, window functions, normalization  
- `05_Python/` â€” Python for data engineering: scripting, Pandas, PySpark basics  
- `06_Azure/` â€” Azure data services (Data Factory, Data Lake, Synapse)  
- `07_Spark_PySpark/` â€” Spark architecture, RDD vs DataFrame, PySpark code snippets  
- `08_Data_Warehousing/` â€” Data-warehousing concepts: star schema, ETL design, OLAP  
- `09_ETL_Pipelines/` â€” End-to-end pipeline examples integrating tools above  
- `10_Projects/` â€” Real-world projects that combine multiple technologies  
- `assets/` â€” Diagrams, images, references  

## ðŸ“… Daily Commit Strategy
Every day Iâ€™ll commit a small piece: a note, a diagram, a code snippet, a project update. Example commit message:  
``Day 14: Added Snowflake stage vs pipe notes``  
This will help build a consistent history and show incremental learning.

## ðŸš€ Tech Stack Focus
AWS | Databricks | Snowflake | PySpark | SQL | Python | Azure | Airflow

